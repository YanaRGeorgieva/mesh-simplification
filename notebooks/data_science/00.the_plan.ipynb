{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The plan\n",
    "####  Written in the end as always"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important\n",
    "Write to me (yanargeorgieva@gmail.com) so I can grant you access to the gdrive where I stored via dvc the mesh data. There is an issue with the new policies of GDrive and that you cannot upload anything in your own gdrive (https://github.com/iterative/dvc/issues/10516). A workaround is using a Google API Console project, which I did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I want to improve the implementation of the Garland&Heckbert simplification algorithm I wrote for the previous module. A TODO is furter optimizations.\n",
    "* To add some small unit tests. I found some bugs with the new code I added and some with the old.\n",
    "* To test it on a dataset. I chose [Thingi10K](https://ten-thousand-models.appspot.com/) which was very ambitious. I ended up from 10K models to just 700 models under ~3MB bacause I had a very small timeframe to do the simplification. I also wanted to do simplification with a greater variety of hyperparameters (`threshold` and `simplification ratio`) but I was satisfied in the end with four combos.\n",
    "* I found out that there is a dataset with geometric data of the [Thingi10K](https://docs.google.com/spreadsheets/d/1ZM5_1ry3Oe5uDJZxQIcFR6fjjas5rX4yjkhQ8p7Kf2Q/edit?gid=1531775051#gid=1531775051) and got excited to extract some features from the simplified meshes. It was hard and with some of the features I had to go with approximations.\n",
    "* I analyzed a bit the big dataset of the original meshes and extracted the subset I needed. The Euler number was rather interesting.\n",
    "* I computed a set of visual fidelity datasets for all runs w.r.t. the original meshes considering 3 error metrics - Hausdorff distance, RMSE and the emtric in the Garland&Heckbert paper.\n",
    "* I \"tried\" (I put it in brackets because it is a try. There is a lot of data and more than one noteboo will be needed to make a better modular analysis of what has happenned after simplification) to analyze all of the generated by me datasets. I hypothesise, there are no universal hyperparameter combo to get the best simplification for every mesh (even for this small dataset and for the 4 coombos I tried)  based on the analysis. I think that there may be some geometric properties of meshes that can be considered as crutial to the choise of the hyperparameters. It will be interesting too see if in future we can create a model which w.r.t. the geometric features of a mesh, can give us a plausible combo of hyperparameters (by plausible I mean with a relatively low error)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
